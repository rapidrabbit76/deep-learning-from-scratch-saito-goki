{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1.2 확률적 경사 하강법(SGD)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1.4 모멘텀\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1.5 AdaGrad\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += gards[key] * gards[key]\n",
    "            params[key] -= self.lr * grads[key]/(np.sqrt(self.h[key])+1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 가중치의 초깃값\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 은닉층의 활성화값 분포\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "x = np.random.randn(1000,100) #1000개의 데이터\n",
    "node_num = 100 #각 은닉층의 노드 수\n",
    "hidden_layer_size = 5 #총 5개의 은닉층\n",
    "activations = {} #활성화 결과(활성화 값)을 저장\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "        \n",
    "    #표준편차가 1일때\n",
    "    # w = np.random.randn(node_num, node_num) * 1\n",
    "    #표준편차를 0.01 일때\n",
    "    # w = np.random.randn(node_num, node_num) * 0.01\n",
    "    #Xavier 초기값을 사용하였을 때\n",
    "    w = np.random.randn(node_num, node_num) / np.sqrt(node_num)\n",
    "    a = np.dot(x,w)\n",
    "    z = sigmoid(a)\n",
    "    activations[i] = z\n",
    "    \n",
    "    \n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1)+\"-layer\")\n",
    "    plt.hist(a.flatten(), 30, range = (0, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 바른 학습을 위해\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 오버피팅\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD,Adam\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True)\n",
    "# 오버피팅 재현하기 위해서 학습 데이터 수를 줄인다\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLayerNet(\n",
    "    input_size=784,\n",
    "    hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "    output_size=10,\n",
    ")\n",
    "# optimizer = SGD(lr=0.01)\n",
    "optimizer = Adam(lr=0.01)\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "pbar = tqdm(range(10000000))\n",
    "for i in pbar:\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        pbar.set_description_str(\n",
    "            f\"E:{epoch_cnt+1}_Itr:{i}, train_acc: {np.mean(train_acc_list): .4f}, test_acc: {np.mean(test_acc_list):.4f}\"\n",
    "        )\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "plt.plot(train_acc_list, label=\"train_acc\")\n",
    "plt.plot(test_acc_list, label=\"test_acc\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 드롭아웃\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x, train_flg = True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape)>self.dropout_ration\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 적절한 하이퍼파라미터 값 찾기\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1 검증 데이터\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import shuffle_dataset\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
    "\n",
    "#훈련 데이터를 셔플\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "\n",
    "#20%를 검증 데이터로 분할\n",
    "validation_rate = 0.2\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련 데이터 : 매개변수 학습  \n",
    "\n",
    "- 검증 데이터 : 하이퍼파라미터의 성능을 평가  \n",
    "\n",
    "- 시험 데이터 : 신경망의 범용 성능을 평가  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.3 하이퍼파라미터 최적화 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.trainer import Trainer\n",
    "\n",
    "def __train(lr, weight_decay, epocs=50):\n",
    "    network = MultiLayerNet(input_size=784,\n",
    "                            hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                            output_size=10, weight_decay_lambda=weight_decay)\n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "                      epochs=epocs, mini_batch_size=100,\n",
    "                      optimizer='sgd',\n",
    "                      optimizer_param={'lr': lr}, verbose=False)\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer.test_acc_list, trainer.train_acc_list\n",
    "\n",
    "optimization_trial = 100\n",
    "results_val = {}\n",
    "results_train = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO 시간이 많이 걸리므로 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(optimization_trial):\n",
    "    weight_decay = 10 ** np.random.uniform(-8,-4)\n",
    "    lr = 10 ** np.random.uniform(-6,-2)\n",
    "    \n",
    "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
    "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num))\n",
    "i = 0\n",
    "\n",
    "for key, val_acc_list in sorted(results_val.items(), key=lambda x: x[1][-1], reverse=True):\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5:\n",
    "        plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, results_train[key], \"--\")\n",
    "    i += 1\n",
    "\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
